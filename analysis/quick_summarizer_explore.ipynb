{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with quick summarizer\n",
    "Quick summarizer - take full text, output summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful for rouge scoring\n",
    "# !pip install -r rouge/requirements.txt\n",
    "# !pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../rfpgo/')\n",
    "from credentials import *\n",
    "from summarize.quick.prompts.quick_field_prompts import *\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.llms import Ollama\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "import json\n",
    "\n",
    "DATA_FP = '../data'\n",
    "LABEL_FP = f'{DATA_FP}/labels'\n",
    "PROMPT_FP = f'{DATA_FP}/prompts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama model\n",
    "fp = 'company_conditions'\n",
    "gemma = Ollama(model=\"gemma:7b\")\n",
    "oai_3 = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "oai_4 = ChatOpenAI(model='gpt-4-turbo')\n",
    "rfps = json.load(open(f'../data/{fp}.json'))\n",
    "content = rfps[0]['prompt'][1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt, llm):\n",
    "    response = llm.invoke(prompt)\n",
    "    if isinstance(response, str):\n",
    "        return response\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = open('../data/prompts/quick_summarize.txt', 'r').read()\n",
    "prompt_w_content = f'{prompt}\\n{content}'\n",
    "response = call_llm(prompt_w_content, oai_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing harness\n",
    "Inputs:\n",
    "- Prompts\n",
    "- llms (for now - just going to set these up with LC)\n",
    "Output:\n",
    "- prompt used\n",
    "- llm used\n",
    "- response\n",
    "- length / total length\n",
    "- rogue score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_rouge(label, response):\n",
    "    from rouge_score import rouge_scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(label, response)\n",
    "    return scores\n",
    "\n",
    "def format_and_output(label, response):\n",
    "    scores = score_rouge(label, response)\n",
    "    output_json = {'label': label, 'response': response, 'scores': scores}\n",
    "    return output_json\n",
    "\n",
    "def run_test(label, llm):\n",
    "    response = call_llm(label, llm)\n",
    "    output_json = format_and_output(label, response)\n",
    "    output_json['model'] = llm.dict()['model']\n",
    "    # TODO: LLM's opinion on the response\n",
    "    return output_json\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "class Summary(object):\n",
    "    def __init__(self, document_fp, label_dict):\n",
    "        self.document_fp = document_fp\n",
    "        self.document = open(document_fp, 'r').read()\n",
    "        if type(label_dict) == str:\n",
    "            label_dict = json.load(open(label_dict))\n",
    "        self.label_dict = label_dict\n",
    "        self.output_dict = defaultdict(dict)\n",
    "\n",
    "    def run(self, llm):\n",
    "        collect_scores = []\n",
    "        for field, label in self.label_dict.items():\n",
    "            field = field.lower()\n",
    "            if field == 'summary':\n",
    "                # summary has a different prompt\n",
    "                prompt = summary_prompt.format(document=self.document)\n",
    "            else:\n",
    "                prompt = field_prompt.format(field=field, document=self.document)\n",
    "            response = call_llm(prompt, llm)\n",
    "            formatted_output = format_and_output(label, response)\n",
    "            collect_scores.append(formatted_output['scores'])\n",
    "            self.output_dict[field]['response'] = formatted_output['response']\n",
    "            self.output_dict[field]['label'] = formatted_output['label']\n",
    "            self.output_dict[field]['scores'] = formatted_output['scores']\n",
    "            # only for debugging\n",
    "            #self.output_dict[field]['prompt'] = prompt\n",
    "\n",
    "        # general request parameters\n",
    "        self.output_dict['model'] = llm.dict()['model']\n",
    "        self.output_dict['document_fp'] = self.document_fp\n",
    "        self.output_dict['overall'] = sum(\n",
    "            [s['rouge1'].fmeasure for s in collect_scores]) / len(collect_scores)\n",
    "    \n",
    "    def save(self, path):\n",
    "        json.dump(self.output_dict, open(path, 'w'), indent=4)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return json.dumps(\n",
    "            {'document': self.document, \n",
    "            'label_dict': self.label_dict, \n",
    "            'output_dict': self.output_dict},\n",
    "            indent=4\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project name\n",
      "agency\n",
      "solicitation number\n",
      "contact person\n",
      "email\n",
      "submission deadline\n",
      "contract term\n",
      "source link\n",
      "summary\n"
     ]
    }
   ],
   "source": [
    "s = Summary(\n",
    "    document_fp=f'{DATA_FP}/0_synth_rfp.txt',\n",
    "    label_dict=f'{LABEL_FP}/howard_09122024/0_summary.json')\n",
    "s.run(llm=gemma)\n",
    "s.save(f'{DATA_FP}/output/howard_09122024/0_summary_output_{gemma.dict()[\"model\"]}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for llm in [oai_3, oai_4]:\n",
    "    for f in range(4):\n",
    "        if os.path.exists(f'{DATA_FP}/output/howard_09122024/{f}_summary_output_{llm.dict()[\"model\"]}.json'): \n",
    "            continue\n",
    "        s = Summary(\n",
    "            document_fp=f'{DATA_FP}/{f}_synth_rfp.txt',\n",
    "            label_dict=f'{LABEL_FP}/howard_09122024/{f}_summary.json'\n",
    "        )\n",
    "        s.run(llm=llm)\n",
    "        s.save(f'{DATA_FP}/output/howard_09122024/{f}_summary_output_{llm.dict()[\"model\"]}.json')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format howard desired output\n",
    "from pathlib import Path\n",
    "fps = Path(f'{LABEL_FP}/howard_09122024/raw').glob('*.txt')\n",
    "for f in fps: \n",
    "    content = open(f, 'r').read()\n",
    "    # summary, split\n",
    "    content = content.replace('Summary: ', 'Summary\\n')\n",
    "    # format as key-value\n",
    "    content = content.split('\\n')[2:]\n",
    "    content = dict(zip(content[0::2], content[1::2]))\n",
    "    # rewrite \"insert deadline\" to not specified\n",
    "    if 'insert deadline' in content['Submission Deadline'].lower(): \n",
    "        content['Submission Deadline'] = 'Not specified'\n",
    "    # output as json\n",
    "    new_path = f.parent.parent / f.with_suffix('.json').name\n",
    "    json.dump(content, open(new_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Name\n",
      "The text does not specify a Project Name, therefore I cannot complete the requested task.\n",
      "Agency/Department/Organization\n",
      "The text does not mention an Agency/Department/Organization in the document, therefore I cannot provide the requested information.\n",
      "Solicitation Number\n",
      "The text does not contain any Solicitation Number, therefore I cannot complete the requested task.\n",
      "Contact Person\n",
      "The text does not specify a Contact Person, therefore I cannot fill in the requested information.\n",
      "Email\n",
      "Sure, here is the email extracted from the document:\n",
      "\n",
      "**Email:** [Insert contact email]\n",
      "Submission Deadline\n",
      "The text does not specify a Submission Deadline, therefore I cannot complete the text.\n",
      "Contract Term\n",
      "The text does not mention a Contract Term, therefore I cannot provide the requested information.\n",
      "Source Link\n",
      "The text does not contain a Source Link, therefore I cannot provide an answer to the question.\n"
     ]
    }
   ],
   "source": [
    "# summarizer as a compound set of fields\n",
    "# template for structured information extraction\n",
    "prompt = \"You are filling in structured information from a document.\\n\\\n",
    "What is the {field} in the document below?\\n\\\n",
    "Do not respond if there is no {field} in the document.\\n\\\n",
    "{document}\\n\\\n",
    "{field}: \"\n",
    "\n",
    "for field in fields:\n",
    "    print(field)\n",
    "    f_prompt = prompt.format(field=field, document=content['Summary'])\n",
    "    response = call_llm(f_prompt, l)\n",
    "    format_and_output(content, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Project Name': 'Wheels Supply for City X Transportation Department', 'Agency': 'City X Transportation Department', 'Solicitation Number': 'Not specified', 'Contact Person': 'Not specified', 'Email': 'Not specified', 'Submission Deadline': '[Insert deadline]', 'Contract Term': 'Not specified', 'Source Link': 'Not provided', 'Summary': 'The City X Transportation Department seeks proposals for eco-friendly, durable wheels for its vehicle fleet. Submission requirements include company background, product specs, pricing, and sustainability practices. Proposals should be submitted electronically to [Insert contact email] with the subject line \"Wheels Supply RFP - City X Transportation Department.\" Evaluations will prioritize sustainability and product quality. For full details, refer to the complete RFP document.'}\n",
      "Sure, here is a summary of the RFP: The City X Transportation Department is seeking proposals for eco-friendly, durable wheels for its vehicle fleet. Proposals must include company background, product specs, pricing, and sustainability practices.\n",
      "{'rouge1': Score(precision=0.8888888888888888, recall=0.24427480916030533, fmeasure=0.3832335329341317), 'rougeL': Score(precision=0.8333333333333334, recall=0.22900763358778625, fmeasure=0.3592814371257485)}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = ['quick_summarize']\n",
    "rfps = [content]\n",
    "llms = [Ollama(model=\"gemma:7b\")]\n",
    "\n",
    "for p, r, l in zip(prompts, rfps, llms):\n",
    "    prompt_w_content = open('../data/prompts/quick_summarize.txt', 'r').read()\n",
    "    prompt_w_content = f'{prompt_w_content}\\n{content}'\n",
    "    format_and_output(content, call_llm(prompt_w_content, l))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.9655172413793104, recall=0.11498973305954825, fmeasure=0.2055045871559633),\n",
       " 'rougeL': Score(precision=0.7931034482758621, recall=0.0944558521560575, fmeasure=0.1688073394495413)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing harness must take a list of "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
